{
  "metadata": {
    "name": "Homework",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(spark.version)\nprint(spark.sparkContext.master)\nspark.range(100000).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrdd \u003d sc.parallelize([1,2,3,4,5])\nrdd.map(lambda x: x * 2).collect()\n\ndf \u003d spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\ndf.selectExpr(\"value * 2 as value\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf \u003d spark.range(1_000_000)\ndf2 \u003d df.filter(\"id % 2 \u003d 0\")\nprint(\"Spark ещё ничего не считал\")\n\ndf2.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\ndata \u003d [\n    (\"user1\",\"A\",100),\n    (\"user1\",\"B\",200),\n    (\"user2\",\"A\",50),\n]\ndf \u003d spark.createDataFrame(data, [\"user\",\"category\",\"amount\"])\ndf.groupBy(\"user\").agg(F.sum(\"amount\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import broadcast\nusers \u003d spark.createDataFrame(\n    [(\"user1\",\"Berlin\"),(\"user2\",\"Munich\")],\n    [\"user\",\"city\"]\n)\norders \u003d spark.createDataFrame(\n    [(\"user1\",100),(\"user2\",300)],\n    [\"user\",\"amount\"]\n)\nusers.join(orders, \"user\").show()\nbroadcast(users).join(orders, \"user\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, desc\ndf \u003d spark.createDataFrame(\n    [(\"user1\",100),(\"user1\",200),(\"user1\",50)],\n    [\"user\",\"amount\"]\n)\nw \u003d Window.partitionBy(\"user\").orderBy(desc(\"amount\"))\ndf.withColumn(\"rn\", row_number().over(w)).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\nbig \u003d spark.range(5_000_000).withColumn(\"x\", (F.col(\"id\") % 10).cast(\"int\"))\nbig.cache()\nbig.count()\nbig.filter(\"x \u003d\u003d 1\").count()\nbig.filter(\"x \u003d\u003d 2\").count()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npath \u003d \"hdfs:///user/zeppelin/spark_demo/\"\ndf \u003d spark.range(10000)\ndf.write.mode(\"overwrite\").parquet(path)\nspark.read.parquet(path).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\nusers \u003d spark.createDataFrame(\n    [\n        (\"u1\", \"Berlin\"),\n        (\"u2\", \"Berlin\"),\n        (\"u3\", \"Munich\"),\n        (\"u4\", \"Hamburg\"),\n    ],\n    [\"user_id\", \"city\"]\n)\norders \u003d spark.createDataFrame(\n    [\n        (\"o1\", \"u1\", \"p1\", 2, 10.0),\n        (\"o2\", \"u1\", \"p2\", 1, 30.0),\n        (\"o3\", \"u2\", \"p1\", 1, 10.0),\n        (\"o4\", \"u2\", \"p3\", 5, 7.0),\n        (\"o5\", \"u3\", \"p2\", 3, 30.0),\n        (\"o6\", \"u3\", \"p3\", 1, 7.0),\n        (\"o7\", \"u4\", \"p1\", 10, 10.0),\n    ],\n    [\"order_id\", \"user_id\", \"product_id\", \"qty\", \"price\"]\n)\nproducts \u003d spark.createDataFrame(\n    [\n        (\"p1\", \"Ring VOLA\"),\n        (\"p2\", \"Ring POROG\"),\n        (\"p3\", \"Ring TISHINA\"),\n    ],\n    [\"product_id\", \"product_name\"]\n)\nusers.show()\norders.show()\nproducts.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.window import Window\n\norders_with_revenue \u003d orders.withColumn(\"revenue\", F.col(\"qty\") * F.col(\"price\"))\n\njoined \u003d orders_with_revenue.join(users, \"user_id\").join(products, \"product_id\")\n\ngrouped \u003d joined.groupBy(\"city\", \"product_id\", \"product_name\").agg(\n    F.count(\"order_id\").alias(\"orders_cnt\"),\n    F.sum(\"qty\").alias(\"qty_sum\"),\n    F.sum(\"revenue\").alias(\"revenue_sum\")\n)\n\nwindow \u003d Window.partitionBy(\"city\").orderBy(F.desc(\"revenue_sum\"))\nresult \u003d grouped.withColumn(\"rank\", F.row_number().over(window)).filter(F.col(\"rank\") \u003c\u003d 2).drop(\"rank\")\n\nresult.orderBy(\"city\", F.desc(\"revenue_sum\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npath \u003d \"hdfs:///tmp/sandbox_zeppelin/mart_city_top_products/\"\n\nresult.write.mode(\"overwrite\").parquet(path)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncheck \u003d spark.read.parquet(\"hdfs:///tmp/sandbox_zeppelin/mart_city_top_products/\")\ncheck.orderBy(\"city\", F.desc(\"revenue_sum\")).show()"
    }
  ]
}